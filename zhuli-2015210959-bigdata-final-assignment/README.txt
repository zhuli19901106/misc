大数据分析与内存计算 期末作业

姓名：朱里
专业：2015级计算机系硕士
学号：2015210959
导师：舒继武教授

写在最前面：运行run-me.sh就能看到结果。前提是你装了python、jdk、hadoop和spark。

这题目说得不清不楚，数据范围多大，每个集合多大，每个元素有什么限制，一样都不说。

既然不说，我也不可能把所有情况都照顾到，所以各种对edge case的优化就不做了。本次作业里我对数据的设定如下
1. 假设两数据集的规模分别为N1、N2，那么N1和N2最多不超过10000。更大的话计算时间就太长了。
2. N1 * N2 控制在差不多十亿的级别，例子里直接把两者设成同一个值。
3. 每个元素就一个字母，每个集合至多26个字母，至少0个字母。
4. 数据集用脚本随机生成，规模可指定。

为了完成本次作业，我用三种方法各搞了一遍。

首先是python版本。因为这个问题不复杂，所以一个python脚本其实就可以解决问题。请看solution-using-only-python文件夹下的内容。为了把多核CPU用起来，我用python的multiprocessing库把程序改进了一下，效果不错。虽然作业没要求这部分，但我拿到问题第一反应就是直接用脚本搞定，索性就这么干了。

然后是hadoop版本。老实说，这问题很不适合用mapreduce来做。本来就是O(N1 * N2)的复杂度，要使用hadoop必须把两个数据集的cartesian product写到文件里去。这么干完全是多此一举。假如N1和N2都是几百万的量级，用mapreduce也没用了，一台电脑的计算能力根本搞不定。所以我认为这作业不适合用mapreduce来写。

最后是spark版北。这个问题很适合用spark做，因为两层循环的过程是计算密集型的，所以完全没必要IO。毕竟内存计算嘛，实际运行速度确实比mapreduce快了很多。

运行须知：
1. cleanup.sh 清除输出文件。
2. run-all.sh 后台运行三个示例，所有内容输出到各自的log中。
3. README.txt 子文件夹里也有。

因为要运行程序得配置环境，各种麻烦。所以我把最近一次本机上运行的小规模数据以及完整日志都留下了，可供查看。
本来打算截图写个完整的实验报告的，想想觉得这个问题比较直白，把配环境的无聊过程讲出来也没什么意思，干脆只写几个README好了。

在此简单描述一下搞这个作业的经过(没错，我是流水账)吧：

准备阶段
1. 系统最近刚崩了，装ubuntu
2. apt-get install 各种工具
3. 装jdk
4. 装hadoop，单点模式(不算艰难，也不太顺利，踩了一些兼容性相关的坑)
5. 装spark，装了就能用(业界良心)
6. 装Eclipse，把hadoop的相关jar包都放一块儿，方便随后写mapreduce程序时引用

Python脚本版
1. 写python版本，单线程版很顺利，效率还过得去
2. 把python版本改成多线程，用了threading库，结果根本没用，因为多线程和多核不一样-_-||
3. 把python版本改成多核，用了multiprocessing库，结果代码写得太挫，速度还不如单线程的
4. 猛然想起操作系统课上讲的“数据相关性”、“锁竞争”，改了两版代码才把效率改上去了，很惭愧，就写了个微小的多核程序
5. 写几个shell脚本，方便之后运行示例
6. python版本搞定

Hadoop版
1. 写mapreduce程序，照着WordCount.java依葫芦画瓢
2. mapreduce要对文件进行split和shuffle，所以没有办法获取一条记录的行号，于是我只能手工给每行加上行号
3. 其余脚本按下不表
4. 运行没问题，但速度是真慢
5. hadoop版本搞定

Spark版
1. 写spark程序，因为常用python，所以毫不犹豫选了pyspark，照着wordcount依葫芦画瓢
2. 运行没问题，速度也很不错
3. 难得这么顺利，UC Berkeley业界良心
4. spark版本搞定

完成作业耗时共两天。

最后，由于这个问题在算法上并不复杂，所以代码层面的细节解释就不写了。
(其实我想过能不能用些数据结构或者算法把复杂度降下去，终归没想出来。)
